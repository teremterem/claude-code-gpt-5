OPENAI_API_KEY=

# OPTIONAL: Set the Anthropic API key if you still want to use Anthropic models
# (which you can do by unsetting some of the remaps below in this file, or by
# explicitly setting them to map back to the Anthropic models).
#ANTHROPIC_API_KEY=

# OPTIONAL: Authentication for the proxy (recommended when exposing the proxy
# beyond your machine). Set a strong random key to require clients to
# authenticate to the LiteLLM proxy.
#
# When set, clients must present this key as their API key when calling the
# proxy. For Claude Code CLI, you can pass it inline like so:
# ```
#   ANTHROPIC_API_KEY="<LITELLM_MASTER_KEY>" \
#   ANTHROPIC_BASE_URL=http://localhost:4000 \
#   claude
# ````
# (If you've previously signed in with Claude Code, run `claude /logout`.)
#LITELLM_MASTER_KEY=

# TODO Update this explanation and the relevant parts of README.md
# RECOMMENDED: You are better off relying on the remaps below, rather than
# setting the desired model in Claude Caude CLI via `claude --model gpt-5-...`
# (even though you could also do that).
#
# The reason being that there are some built-in agents in Claude Code that do
# not inherit the model that was chosen globaly for the CLI and instead are
# hardwired to always use specific models by Anthropic.
#
# (If you do not want to use these remaps but also want to avoid getting
# warnings, then, instead of commenting them out, simply set them to empty
# strings.)
#REMAP_CLAUDE_HAIKU_TO=gpt-5-mini-reason-minimal
#REMAP_CLAUDE_SONNET_TO=gpt-5-codex-reason-medium
#REMAP_CLAUDE_OPUS_TO=gpt-5-codex-reason-high

# OPTIONAL: Override base URLs if needed
#OPENAI_BASE_URL=https://api.openai.com/v1
#ANTHROPIC_BASE_URL=https://api.anthropic.com

# RECOMMENDED: Keep the setting below `true` (or unset, which is equivalent).
# Claude Code CLI cannot handle multiple tool calls in a single model response,
# which GPT-5 with medium/high reasoning effort may attempt.
#ENFORCE_ONE_TOOL_CALL_PER_RESPONSE=true

# OPTIONAL: Whether to always convert ChatCompletions API interactions to
# Responses API, regardless of the model (true), or only when necessary (false
# or unset, aka default - RECOMMENDED).
#
# In the latter case, Responses API will still be used if the model is
# GPT-5-Codex, as it does not support ChatCompletions API (otherwise, no
# conversion will occur).
#ALWAYS_USE_RESPONSES_API=false

# OPTIONAL: Langfuse configuration for logging LiteLLM request/response traces.
# Useful for debugging.
#
# NOTE: If you set these keys, make sure to install Langfuse with either
# `uv sync --all-extras` or `uv sync --extra langfuse`
#LANGFUSE_SECRET_KEY="sk-..."
#LANGFUSE_PUBLIC_KEY="pk-..."
#LANGFUSE_HOST="https://cloud.langfuse.com"

# OPTIONAL: Alternative logging of LiteLLM request/response traces (as well as
# potential conversions between ChatCompletions API and Responses API) in the
# form of local markdown files written to `.traces/` folder. Makes it easier to
# feed the traces into AI Coding Assistants to fix things.
#WRITE_TRACES_TO_FILES=false
