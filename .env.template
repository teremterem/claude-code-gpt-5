# LLM provider specific environment variables. Check out the following resource
# to learn more about all the providers (and, consequently, environment
# variables) LiteLLM supports: https://docs.litellm.ai/docs/providers
OPENAI_API_KEY=

# OPTIONAL: Set the Anthropic API key if you still want to use Anthropic models
# (see the explanation to the REMAP_* variables below).
#ANTHROPIC_API_KEY=

# OPTIONAL: Override base URLs if needed
#OPENAI_BASE_URL=https://api.openai.com/v1
#ANTHROPIC_BASE_URL=https://api.anthropic.com

# OPTIONAL: Authentication for the LiteLLM server (recommended when exposing
# the server beyond your machine). Set a strong random key to require clients
# to authenticate to the LiteLLM server.
#
# When set, clients must present this key as their API key when calling the
# proxy. For Claude Code CLI, you can pass it inline like so:
# ```
#   ANTHROPIC_API_KEY="<LITELLM_MASTER_KEY>" \
#   ANTHROPIC_BASE_URL=http://localhost:4000 \
#   claude
# ```
# (If you've previously signed in with Claude Code CLI, run `claude /logout`.)
#LITELLM_MASTER_KEY=

# OPTIONAL: Change model remaps if you need to (the values you see below are
# the defaults).
#
# Also, if you do not want to remap one or more Claude models at all and want
# to keep using the original ones by Anthropic, uncomment the respective
# variables, but delete their values (leave nothing to the right of the equal
# signs), or explicitly set them to map back to the Anthropic models.
#
# NOTE: Remapping is done not only for Claude Haiku and Sonnet, but also for
# Opus, because /agents in Claude Code CLI don't always inherit the model
# choice made for the CLI globally, they can also be configured individually.
# Moreover, the model choices for the built-in agents are hardcoded and cannot
# be changed. For these reasons it is "safer" to remap all models.
#REMAP_CLAUDE_HAIKU_TO=gpt-5-mini-reason-minimal
#REMAP_CLAUDE_SONNET_TO=gpt-5-reason-medium
#REMAP_CLAUDE_OPUS_TO=gpt-5-reason-high

# OPTIONAL: You can turn off the prompt injection that forces non-Claude models
# to use only one tool at a time.
#
# ATTENTION: Turning it off is NOT recommended. GPT-5, when used with medium or
# high reasoning effort and without such injection, attempts to make multiple
# tool calls at once quite often, and that causes Claude Code CLI to silently
# stop processing the request (the CLI does not support multiple tool calls in
# a single response).
#ENFORCE_ONE_TOOL_CALL_PER_RESPONSE=false

# OPTIONAL: Whether to always convert ChatCompletions API interactions to
# Responses API, regardless of the model (true), or only when necessary (false
# or unset).
#
# Currently, when the env var below is set to false or unset (RECOMMENDED),
# Responses API will be used only with GPT-5-Codex, as this model does not
# support ChatCompletions API, with other models no conversion will occur.
#
# ATTENTION: Support of Responses API (and, subsequently, GPT-5-Codex) is still
# WORK IN PROGRESS and is completely unreliable. This will be addressed in
# future versions (at which point this paragraph of the comment will be
# removed).
#ALWAYS_USE_RESPONSES_API=true

# OPTIONAL: Langfuse configuration for logging LiteLLM request/response traces.
# Useful for debugging.
#
# NOTE: If you set these keys, make sure to install Langfuse with either
# `uv sync --all-extras` or `uv sync --extra langfuse`
#LANGFUSE_SECRET_KEY="sk-..."
#LANGFUSE_PUBLIC_KEY="pk-..."
#LANGFUSE_HOST="https://cloud.langfuse.com"

# OPTIONAL: Alternative logging of LiteLLM request/response traces (as well as
# potential conversions between ChatCompletions API and Responses API) in the
# form of local markdown files written to `.traces/` folder. Makes it easier to
# feed the traces into AI Coding Assistants to fix things.
#WRITE_TRACES_TO_FILES=true

PYTHONUNBUFFERED=1
