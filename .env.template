OPENAI_API_KEY=

# OPTIONAL: Set the Anthropic API key if you still want to use Anthropic models
# (which you can do by unsetting some of the remaps below in this file, or by
# explicitly setting them to map back to the Anthropic models).
#ANTHROPIC_API_KEY=

# RECOMMENDED: You are better off relying on the remaps below, rather than
# setting the desired model in Claude Caude CLI via `claude --model gpt-5-...`
# (even though you could also do that).
#
# The reason being that there are some built-in agents in Claude Code that do
# not inherit the model that was chosen globaly for the CLI and instead are
# hardwired to always use specific models by Anthropic.
#
# (If you do not want to use these remaps but also want to avoid getting
# warnings, then, instead of commenting them out, simply set them to empty
# strings.)
REMAP_CLAUDE_HAIKU_TO=gpt-5-nano-reason-minimal
REMAP_CLAUDE_SONNET_TO=gpt-5-reason-medium
REMAP_CLAUDE_OPUS_TO=gpt-5-reason-high

# TODO Explain this key here in this file
# TODO Pass this env var through in the docker compose file
# TODO Elaborate on it in DOCKER_DEPLOYMENT.md
# TODO In all cases mention ANTHROPIC_API_KEY= env var that can be prepended before `claude` CLI command (but the user will need to logout of Claude Code if logged in already)
#LITELLM_MASTER_KEY=

# OPTIONAL: Override base URLs if needed
#OPENAI_BASE_URL=https://api.openai.com/v1
#ANTHROPIC_BASE_URL=https://api.anthropic.com

# RECOMMENDED: Keep the setting below `true` (or unset, which is equivalent).
# Claude Code CLI cannot handle multiple tool calls in a single model response,
# which GPT-5 with medium/high reasoning effort may attempt.
#ENFORCE_ONE_TOOL_CALL_PER_RESPONSE=true

# OPTIONAL: Langfuse configuration for logging all LiteLLM requests/responses.
# Useful for debugging.
#
# NOTE: If you set these keys, make sure to install Langfuse with either
# `uv sync --all-extras` or `uv sync --extra langfuse`
#LANGFUSE_SECRET_KEY="sk-..."
#LANGFUSE_PUBLIC_KEY="pk-..."
#LANGFUSE_HOST="https://cloud.langfuse.com"
